{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis - NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roht20/Portfolio/blob/master/Sentiment_Analysis_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "svMkZMeS69xR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis **"
      ]
    },
    {
      "metadata": {
        "id": "PcIzzuus7Ldp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's import the necessary libraries and set a random seed to be able to reproduce the same set of splits and values in the code"
      ]
    },
    {
      "metadata": {
        "id": "ffgyilQv62aL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcO6qGazzV2X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6efc8b7b-01f6-4deb-a2ac-361b6be1906b"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('labeledTrainData.tsv',header=0, delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"5814_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"With all this stuff going down at the moment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2381_9\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"7759_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"3630_4\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"It must be assumed that those who praised thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"9495_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  sentiment                                             review\n",
              "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
              "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
              "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
              "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
              "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "metadata": {
        "id": "m5-LDQ6M7ih2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af1d89ed-e63c-43c0-ca8e-6d7f452658e5"
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "metadata": {
        "id": "VXr97EQP4B1V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset is of the shape 25000 x 3"
      ]
    },
    {
      "metadata": {
        "id": "-3T6g_mm71-5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's us split the data into training and testing with the ratio 80:20"
      ]
    },
    {
      "metadata": {
        "id": "TIfu6tmR39Uz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['review'],\n",
        "    df['sentiment'],\n",
        "    test_size=0.2, \n",
        "    random_state=42\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4lchRUpI4AV9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9b3fa7c-6b23-4e67-b39b-8582874ad093"
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "metadata": {
        "id": "uBppxBFN4R6w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Prepare Data\n",
        "\n",
        "1.Convert reviews to Number sequences using Tokenizer"
      ]
    },
    {
      "metadata": {
        "id": "sABg78Zj4O01",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Considering top 5000 words = Vocablury size\n",
        "top_words = 5000\n",
        "t = Tokenizer(num_words=top_words)\n",
        "\n",
        "#Fit tokenizer of training data\n",
        "t.fit_on_texts(X_train.tolist())\n",
        "\n",
        "#Get the word index for each of the word in the review\n",
        "X_train = t.texts_to_sequences(X_train.tolist())\n",
        "X_test = t.texts_to_sequences(X_test.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C9OAOSqC4YLn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "86d67a86-bea7-4cfd-f706-a47571efa7bb"
      },
      "cell_type": "code",
      "source": [
        "print('Length of review# 32 is: ', len(X_train[32]))\n",
        "print('Length of review# 1208 is: ', len(X_train[1208]))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of review# 32 is:  317\n",
            "Length of review# 1208 is:  117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PZEH4BPe8241",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**As seen above comparing two random documents the length varies, so let's standardize the length of each of the document using padding**"
      ]
    },
    {
      "metadata": {
        "id": "KpbIbSEb5CkR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "\n",
        "#Length for each review\n",
        "max_review_length = 300\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train,maxlen=max_review_length,\n",
        "                                 padding='post')\n",
        "\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, \n",
        "                                padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d_J4z5EU9W7w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Now comparing the length of the same documents as above, we should get exactly 300 as thats the max_review_length chosen**"
      ]
    },
    {
      "metadata": {
        "id": "C-M0SGke5F5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "94e84cf4-c772-4894-9ec1-88bca2ec8b83"
      },
      "cell_type": "code",
      "source": [
        "print('Length of review# 32 is: ', len(X_train[32]))\n",
        "print('Length of review# 1208 is: ', len(X_train[1208]))"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of review# 32 is:  300\n",
            "Length of review# 1208 is:  300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6taaS_W84ROi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Just to reconfirm below, the values have been padded with 0's to make it a standard size**"
      ]
    },
    {
      "metadata": {
        "id": "YRG0XJsp4c6D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "edbd1818-f1b6-44b9-ff93-fe26501427c8"
      },
      "cell_type": "code",
      "source": [
        "X_train[1208]"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  11,   17,    6,    3,  977,    3,   62,    4,    3,  183,  251,\n",
              "        311,    1,  317,    2,    9,   63,  585,   21,  622,   14,    1,\n",
              "         17,   18,    9,    6,    3,   82,   62,   10,   13, 4142,   31,\n",
              "         11,   19,    1,  112,    2,    1,   62,  117,   82,   10,   37,\n",
              "         11,   19,   85,    9,    6,    3,  278,   62,   42,   68,    3,\n",
              "        543,   12,   10,   13,   46,    2,   10,  229,  788,   15,    1,\n",
              "         12,    6,  396,   85,   34,  485,    5,  127,  130,  111,   12,\n",
              "         94,   10,  383,   12, 1441,   25,    5,   64,   11,   19,  318,\n",
              "          1,  183,  657,    2,   31,    1,  845,  138,   36,   11,   19,\n",
              "         11,   19,   22,   67, 1631,    1,   17, 1689,   36, 4960,   39,\n",
              "         98,  143,   62,   12,  563,    8,    1,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "metadata": {
        "id": "kY7wse0s4mvs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Its time to Build the Graph**"
      ]
    },
    {
      "metadata": {
        "id": "bw9tzs7R4qg_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importing the required libraries"
      ]
    },
    {
      "metadata": {
        "id": "RRpbHhnA4fmG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dropout, Dense, Embedding, Flatten, LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DE6f_0zR-Lh3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Here I am choosing the vocabulory size of 50 ie each word is represented by 50 numbers**"
      ]
    },
    {
      "metadata": {
        "id": "uaewDTwr4tQE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_vector_length = 50 \n",
        "\n",
        "# Instantiating the sequential model\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JRk25yIv4x-J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Adding Embedding Layer**"
      ]
    },
    {
      "metadata": {
        "id": "UqYEm0YW4vaP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(\n",
        "    Embedding(top_words+1, \n",
        "                    embedding_vector_length, \n",
        "                    input_length=max_review_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dz6vp04S5eRx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Output from Embedding is 3 dimension \n",
        "batch_size x max_review_length x embedding_vector_length. \n",
        "We need to flatten the output for Dense layer **"
      ]
    },
    {
      "metadata": {
        "id": "f2Y3ZNdQ5Xza",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Flattening the output from Embedding layer to feed it into the Dense Layer\n",
        "model.add(Flatten())\n",
        "\n",
        "#Dense Layers - Fully Connected Layer with 3 hidden layers and activation function used 'relu'\n",
        "model.add(Dense(200,activation='relu'))\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dense(60,activation='relu'))\n",
        "model.add(Dense(30,activation='relu'))\n",
        "\n",
        "#Output layer - Single value is output and activation function used is sigmoid which travels from 0 to 1\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "giEVyXT4AoKT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Finally let's fit the model**"
      ]
    },
    {
      "metadata": {
        "id": "h6CGiuXL5mDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "9d9daca5-34d3-44d6-c73d-04475c39d915"
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train,y_train,\n",
        "          epochs=10,\n",
        "          batch_size=128,\n",
        "          shuffle=True, \n",
        "          validation_data=(X_test, y_test))"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 17s 853us/step - loss: 0.5049 - acc: 0.7215 - val_loss: 0.3241 - val_acc: 0.8646\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 15s 765us/step - loss: 0.1712 - acc: 0.9361 - val_loss: 0.3374 - val_acc: 0.8632\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 15s 743us/step - loss: 0.0333 - acc: 0.9900 - val_loss: 0.6544 - val_acc: 0.8406\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 15s 743us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.8198 - val_acc: 0.8504\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 15s 741us/step - loss: 0.0164 - acc: 0.9941 - val_loss: 0.6541 - val_acc: 0.8462\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 15s 742us/step - loss: 0.0174 - acc: 0.9940 - val_loss: 0.7224 - val_acc: 0.8506\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 15s 745us/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.9408 - val_acc: 0.8470\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 15s 742us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.9800 - val_acc: 0.8480\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 15s 742us/step - loss: 0.0045 - acc: 0.9984 - val_loss: 1.1292 - val_acc: 0.8276\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 15s 742us/step - loss: 0.0094 - acc: 0.9967 - val_loss: 0.9857 - val_acc: 0.8360\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd219734208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "metadata": {
        "id": "FSk_HDlxA57r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**With an accuracy of 99.67% on trained data and validation accuracy of 83.60% our model seems to be performing just about okay, however we can improve the model using different techniques such as dropout to avoid overfitting, LSTM and pre-embedding techniques**"
      ]
    },
    {
      "metadata": {
        "id": "5noIMJEQBNRj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Let's us try to predict and check if a particular document reflects either positive or negative sentiment with the default threshold of value 0.5**"
      ]
    },
    {
      "metadata": {
        "id": "Imc8qRi35o0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "39e26762-903a-4708-85a9-9c1dba729fd2"
      },
      "cell_type": "code",
      "source": [
        "model.predict(X_test[0:2])"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.4967514e-07],\n",
              "       [9.6575123e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "metadata": {
        "id": "YJYalv0eMFSC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Seems like both the first 2 documents reflect negative sentiments as the values are less than the set threshold**"
      ]
    },
    {
      "metadata": {
        "id": "gHnk_tgj6AUO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}